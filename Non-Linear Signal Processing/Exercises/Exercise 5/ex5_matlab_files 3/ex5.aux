\relax 
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Neural network with one hidden layer.}}{1}}
\newlabel{NN.eps}{{1}{1}}
\newlabel{eq:model}{{1}{1}}
\newlabel{eq:model}{{3}{2}}
\newlabel{eq:error1}{{5}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The weight update is done by gradient descent going the opposite direction of the cost functions gradient.}}{2}}
\newlabel{gradient.eps}{{2}{2}}
\newlabel{eq:gradient}{{6}{2}}
\newlabel{eq:gradient}{{7}{2}}
\newlabel{eq:output}{{8}{3}}
\newlabel{eq:hidden}{{9}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The nonlinear function $g(\cdot ) = \qopname  \relax o{tanh}(\cdot )$.}}{3}}
\newlabel{tanh.eps}{{3}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Some optimization algorithms.}}{4}}
\newlabel{tab:optim}{{1}{4}}
\newlabel{eq:step}{{10}{5}}
\newlabel{eq:backprob}{{11}{5}}
\newlabel{eq:taylor}{{12}{5}}
\newlabel{eq:derivative}{{13}{5}}
\newlabel{eq:zero}{{14}{5}}
\newlabel{eq:pseudogn}{{15}{5}}
\newlabel{eq:conj}{{16}{5}}
\newlabel{eq:conjgrad}{{17}{5}}
